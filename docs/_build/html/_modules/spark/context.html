

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>spark.context &mdash; Splice MLManager  documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> Splice MLManager
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../splicemachine.html">splicemachine package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Splice MLManager</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>spark.context</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for spark.context</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Copyright 2020 Splice Machine, Inc.</span>

<span class="sd">Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="sd">you may not use this file except in compliance with the License.</span>
<span class="sd">You may obtain a copy of the License at</span>

<span class="sd">    http://www.apache.org/licenses/LICENSE-2.0</span>

<span class="sd">Unless required by applicable law or agreed to in writing, software</span>
<span class="sd">distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="sd">WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="sd">See the License for the specific language governing permissions and</span>
<span class="sd">limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">py4j.java_gateway</span> <span class="kn">import</span> <span class="n">java_import</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">_parse_datatype_json_string</span>
<span class="kn">from</span> <span class="nn">splicemachine.spark.constants</span> <span class="kn">import</span> <span class="n">CONVERSIONS</span>


<div class="viewcode-block" id="PySpliceContext"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext">[docs]</a><span class="k">class</span> <span class="nc">PySpliceContext</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class implements a SpliceMachineContext object (similar to the SparkContext object)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_spliceSparkPackagesName</span> <span class="o">=</span> <span class="s2">&quot;com.splicemachine.spark.splicemachine.*&quot;</span>

    <span class="k">def</span> <span class="nf">_splicemachineContext</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">com</span><span class="o">.</span><span class="n">splicemachine</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">splicemachine</span><span class="o">.</span><span class="n">SplicemachineContext</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jdbcurl</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparkSession</span><span class="p">,</span> <span class="n">JDBC_URL</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_unit_testing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param JDBC_URL: (string) The JDBC URL Connection String for your Splice Machine Cluster</span>
<span class="sd">        :param sparkSession: (sparkContext) A SparkSession object for talking to Spark</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">JDBC_URL</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">jdbcurl</span> <span class="o">=</span> <span class="n">JDBC_URL</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">jdbcurl</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;BEAKERX_SQL_DEFAULT_JDBC&#39;</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">KeyError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                    <span class="s2">&quot;Could not locate JDBC URL. If you are not running on the cloud service,&quot;</span>
                    <span class="s2">&quot;please specify the JDBC_URL=&lt;some url&gt; keyword argument in the constructor&quot;</span>
                <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_unit_testing</span> <span class="o">=</span> <span class="n">_unit_testing</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">_unit_testing</span><span class="p">:</span>  <span class="c1"># Private Internal Argument to Override Using JVM</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">spark_sql_context</span> <span class="o">=</span> <span class="n">sparkSession</span><span class="o">.</span><span class="n">_wrapped</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">spark_session</span> <span class="o">=</span> <span class="n">sparkSession</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_sql_context</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_jvm</span>
            <span class="n">java_import</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spliceSparkPackagesName</span><span class="p">)</span>
            <span class="n">java_import</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="s2">&quot;org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions&quot;</span><span class="p">)</span>
            <span class="n">java_import</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="s2">&quot;org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils&quot;</span><span class="p">)</span>
            <span class="n">java_import</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="s2">&quot;scala.collection.JavaConverters._&quot;</span><span class="p">)</span>
            <span class="n">java_import</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="s2">&quot;com.splicemachine.derby.impl.*&quot;</span><span class="p">)</span>
            <span class="n">java_import</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="s1">&#39;org.apache.spark.api.python.PythonUtils&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">com</span><span class="o">.</span><span class="n">splicemachine</span><span class="o">.</span><span class="n">derby</span><span class="o">.</span><span class="n">impl</span><span class="o">.</span><span class="n">SpliceSpark</span><span class="o">.</span><span class="n">setContext</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">spark_sql_context</span><span class="o">.</span><span class="n">_jsc</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_splicemachineContext</span><span class="p">()</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">.tests.mocked</span> <span class="kn">import</span> <span class="n">MockedScalaContext</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">spark_sql_context</span> <span class="o">=</span> <span class="n">sparkSession</span><span class="o">.</span><span class="n">_wrapped</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">spark_session</span> <span class="o">=</span> <span class="n">sparkSession</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">MockedScalaContext</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jdbcurl</span><span class="p">)</span>

<div class="viewcode-block" id="PySpliceContext.toUpper"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.toUpper">[docs]</a>    <span class="k">def</span> <span class="nf">toUpper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a dataframe with all of the columns in uppercase</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe to convert to uppercase</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span>
            <span class="n">s</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        <span class="c1"># You need to re-generate the dataframe for the capital letters to take effect</span>
        <span class="k">return</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.replaceDataframeSchema"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.replaceDataframeSchema">[docs]</a>    <span class="k">def</span> <span class="nf">replaceDataframeSchema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a dataframe with all column names replaced with the proper string case from the DB table</span>

<span class="sd">        :param dataframe: (Dataframe) A dataframe with column names to convert</span>
<span class="sd">        :param schema_table_name: (str) The schema.table with the correct column cases to pull from the database</span>
<span class="sd">        :return: (DataFrame) A Spark DataFrame with the replaced schema</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getSchema</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">)</span>
        <span class="c1"># Fastest way to replace the column case if changed</span>
        <span class="n">dataframe</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dataframe</span></div>

<div class="viewcode-block" id="PySpliceContext.getConnection"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.getConnection">[docs]</a>    <span class="k">def</span> <span class="nf">getConnection</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a connection to the database</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">getConnection</span><span class="p">()</span></div>

<div class="viewcode-block" id="PySpliceContext.tableExists"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.tableExists">[docs]</a>    <span class="k">def</span> <span class="nf">tableExists</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_and_or_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check whether or not a table exists</span>

<span class="sd">        :Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                splice.tableExists(&#39;schemaName.tableName&#39;)\n</span>
<span class="sd">                # or\n</span>
<span class="sd">                splice.tableExists(&#39;schemaName&#39;, &#39;tableName&#39;)</span>

<span class="sd">        :param schema_and_or_table_name: (str) Pass the schema name in this param when passing the table_name param,</span>
<span class="sd">          or pass schemaName.tableName in this param without passing the table_name param</span>
<span class="sd">        :param table_name: (optional) (str) Table Name, used when schema_and_or_table_name contains only the schema name</span>
<span class="sd">        :return: (bool) whether or not the table exists</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">table_name</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">tableExists</span><span class="p">(</span><span class="n">schema_and_or_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">tableExists</span><span class="p">(</span><span class="n">schema_and_or_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.dropTable"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.dropTable">[docs]</a>    <span class="k">def</span> <span class="nf">dropTable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_and_or_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Drop a specified table.</span>

<span class="sd">        :Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                splice.dropTable(&#39;schemaName.tableName&#39;) \n</span>
<span class="sd">                # or\n</span>
<span class="sd">                splice.dropTable(&#39;schemaName&#39;, &#39;tableName&#39;)</span>

<span class="sd">        :param schema_and_or_table_name: (str) Pass the schema name in this param when passing the table_name param,</span>
<span class="sd">          or pass schemaName.tableName in this param without passing the table_name param</span>
<span class="sd">        :param table_name: (optional) (str) Table Name, used when schema_and_or_table_name contains only the schema name</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">table_name</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">dropTable</span><span class="p">(</span><span class="n">schema_and_or_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">dropTable</span><span class="p">(</span><span class="n">schema_and_or_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.df"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.df">[docs]</a>    <span class="k">def</span> <span class="nf">df</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sql</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a Spark Dataframe from the results of a Splice Machine SQL Query</span>

<span class="sd">        :Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                df = splice.df(&#39;SELECT * FROM MYSCHEMA.TABLE1 WHERE COL2 &gt; 3&#39;)</span>

<span class="sd">        :param sql: (str) SQL Query (eg. SELECT * FROM table1 WHERE col2 &gt; 3)</span>
<span class="sd">        :return: (Dataframe) A Spark DataFrame containing the results</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">df</span><span class="p">(</span><span class="n">sql</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_sql_context</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.insert"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.insert">[docs]</a>    <span class="k">def</span> <span class="nf">insert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">to_upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Insert a dataframe into a table (schema.table).</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe you would like to insert</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to insert the DF</span>
<span class="sd">        :param to_upper: (bool) If the dataframe columns should be converted to uppercase before table creation</span>
<span class="sd">                            If False, the table will be created with lower case columns. [Default False]</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">to_upper</span><span class="p">:</span>
            <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toUpper</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.insertWithStatus"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.insertWithStatus">[docs]</a>    <span class="k">def</span> <span class="nf">insertWithStatus</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">statusDirectory</span><span class="p">,</span> <span class="n">badRecordsAllowed</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Insert a dataframe into a table (schema.table) while tracking and limiting records that fail to insert.</span>
<span class="sd">        The status directory and number of badRecordsAllowed allow for duplicate primary keys to be</span>
<span class="sd">        written to a bad records file.  If badRecordsAllowed is set to -1, all bad records will be written</span>
<span class="sd">        to the status directory.</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe you would like to insert</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to insert the dataframe</span>
<span class="sd">        :param statusDirectory: (str) The status directory where bad records file will be created</span>
<span class="sd">        :param badRecordsAllowed: (int) The number of bad records are allowed. -1 for unlimited</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replaceDataframeSchema</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">statusDirectory</span><span class="p">,</span> <span class="n">badRecordsAllowed</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.insertRdd"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.insertRdd">[docs]</a>    <span class="k">def</span> <span class="nf">insertRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Insert an rdd into a table (schema.table)</span>

<span class="sd">        :param rdd: (RDD) The RDD you would like to insert</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to insert the RDD</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.insertRddWithStatus"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.insertRddWithStatus">[docs]</a>    <span class="k">def</span> <span class="nf">insertRddWithStatus</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">statusDirectory</span><span class="p">,</span> <span class="n">badRecordsAllowed</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Insert an rdd into a table (schema.table) while tracking and limiting records that fail to insert. \</span>
<span class="sd">        The status directory and number of badRecordsAllowed allow for duplicate primary keys to be \</span>
<span class="sd">        written to a bad records file.  If badRecordsAllowed is set to -1, all bad records will be written \</span>
<span class="sd">        to the status directory.</span>

<span class="sd">        :param rdd: (RDD) The RDD you would like to insert</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to insert the dataframe</span>
<span class="sd">        :param statusDirectory: (str) The status directory where bad records file will be created</span>
<span class="sd">        :param badRecordsAllowed: (int) The number of bad records are allowed. -1 for unlimited</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">insertWithStatus</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span><span class="p">,</span>
            <span class="n">statusDirectory</span><span class="p">,</span>
            <span class="n">badRecordsAllowed</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.upsert"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.upsert">[docs]</a>    <span class="k">def</span> <span class="nf">upsert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Upsert the data from a dataframe into a table (schema.table).</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe you would like to upsert</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to upsert the RDD</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># make sure column names are in the correct case</span>
        <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replaceDataframeSchema</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">upsert</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.upsertWithRdd"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.upsertWithRdd">[docs]</a>    <span class="k">def</span> <span class="nf">upsertWithRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Upsert the data from an RDD into a table (schema.table).</span>

<span class="sd">        :param rdd: (RDD) The RDD you would like to upsert</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to upsert the RDD</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">upsert</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.delete"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.delete">[docs]</a>    <span class="k">def</span> <span class="nf">delete</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Delete records in a dataframe based on joining by primary keys from the data frame.</span>
<span class="sd">        Be careful with column naming and case sensitivity.</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe you would like to delete</span>
<span class="sd">        :param schema_table_name: (str) Splice Machine Table</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.deleteWithRdd"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.deleteWithRdd">[docs]</a>    <span class="k">def</span> <span class="nf">deleteWithRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Delete records using an rdd based on joining by primary keys from the rdd.</span>
<span class="sd">        Be careful with column naming and case sensitivity.</span>

<span class="sd">        :param rdd: (RDD) The RDD containing the primary keys you would like to delete from the table</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) Splice Machine Table</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.update"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update data from a dataframe for a specified schema_table_name (schema.table).</span>
<span class="sd">        The keys are required for the update and any other columns provided will be updated</span>
<span class="sd">        in the rows.</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe you would like to update</span>
<span class="sd">        :param schema_table_name: (str) Splice Machine Table</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># make sure column names are in the correct case</span>
        <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replaceDataframeSchema</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.updateWithRdd"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.updateWithRdd">[docs]</a>    <span class="k">def</span> <span class="nf">updateWithRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update data from an rdd for a specified schema_table_name (schema.table).</span>
<span class="sd">        The keys are required for the update and any other columns provided will be updated</span>
<span class="sd">        in the rows.</span>

<span class="sd">        :param rdd: (RDD) The RDD you would like to use for updating the table</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) Splice Machine Table</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.getSchema"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.getSchema">[docs]</a>    <span class="k">def</span> <span class="nf">getSchema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the schema via JDBC.</span>

<span class="sd">        :param schema_table_name: (str) Table name</span>
<span class="sd">        :return: (StructType) PySpark StructType representation of the table</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_parse_datatype_json_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">getSchema</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">)</span><span class="o">.</span><span class="n">json</span><span class="p">())</span></div>

<div class="viewcode-block" id="PySpliceContext.execute"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.execute">[docs]</a>    <span class="k">def</span> <span class="nf">execute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_string</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        execute a query over JDBC</span>

<span class="sd">        :Example:</span>
<span class="sd">            .. code-block:: python</span>
<span class="sd">            </span>
<span class="sd">                splice.execute(&#39;DELETE FROM TABLE1 WHERE col2 &gt; 3&#39;)</span>

<span class="sd">        :param query_string: (str) SQL Query (eg. SELECT * FROM table1 WHERE col2 &gt; 3)</span>
<span class="sd">        :return: None</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">query_string</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.executeUpdate"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.executeUpdate">[docs]</a>    <span class="k">def</span> <span class="nf">executeUpdate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_string</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        execute a dml query:(update,delete,drop,etc)</span>

<span class="sd">        :Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                splice.executeUpdate(&#39;DROP TABLE table1&#39;)</span>

<span class="sd">        :param query_string: (string) SQL Query (eg. DROP TABLE table1)</span>
<span class="sd">        :return: None</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">executeUpdate</span><span class="p">(</span><span class="n">query_string</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.internalDf"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.internalDf">[docs]</a>    <span class="k">def</span> <span class="nf">internalDf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_string</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        SQL to Dataframe translation (Lazy). Runs the query inside Splice Machine and sends the results to the Spark Adapter app</span>

<span class="sd">        :param query_string: (str) SQL Query (eg. SELECT * FROM table1 WHERE col2 &gt; 3)</span>
<span class="sd">        :return: (DataFrame) pyspark dataframe contains the result of query_string</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">internalDf</span><span class="p">(</span><span class="n">query_string</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_sql_context</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.rdd"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.rdd">[docs]</a>    <span class="k">def</span> <span class="nf">rdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">column_projection</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Table with projections in Splice mapped to an RDD.</span>

<span class="sd">        :param schema_table_name: (string) Accessed table</span>
<span class="sd">        :param column_projection: (list of strings) Names of selected columns</span>
<span class="sd">        :return: (RDD[Row]) the result of the projection</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">column_projection</span><span class="p">:</span>
            <span class="n">colnames</span> <span class="o">=</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">column_projection</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">colnames</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">(</span><span class="s1">&#39;select &#39;</span><span class="o">+</span><span class="n">colnames</span><span class="o">+</span><span class="s1">&#39; from &#39;</span><span class="o">+</span><span class="n">schema_table_name</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span></div>

<div class="viewcode-block" id="PySpliceContext.internalRdd"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.internalRdd">[docs]</a>    <span class="k">def</span> <span class="nf">internalRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">column_projection</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Table with projections in Splice mapped to an RDD.</span>
<span class="sd">        Runs the projection inside Splice Machine and sends the results to the Spark Adapter app as an rdd</span>

<span class="sd">        :param schema_table_name: (str) Accessed table</span>
<span class="sd">        :param column_projection: (list of strings) Names of selected columns</span>
<span class="sd">        :return: (RDD[Row]) the result of the projection</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">column_projection</span><span class="p">:</span>
            <span class="n">colnames</span> <span class="o">=</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">column_projection</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">colnames</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">internalDf</span><span class="p">(</span><span class="s1">&#39;select &#39;</span><span class="o">+</span><span class="n">colnames</span><span class="o">+</span><span class="s1">&#39; from &#39;</span><span class="o">+</span><span class="n">schema_table_name</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span></div>

<div class="viewcode-block" id="PySpliceContext.truncateTable"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.truncateTable">[docs]</a>    <span class="k">def</span> <span class="nf">truncateTable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Truncate a table</span>

<span class="sd">        :param schema_table_name: (str) the full table name in the format &quot;schema.table_name&quot; which will be truncated</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">truncateTable</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.analyzeSchema"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.analyzeSchema">[docs]</a>    <span class="k">def</span> <span class="nf">analyzeSchema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Analyze the schema</span>

<span class="sd">        :param schema_name: (str) schema name which stats info will be collected</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">analyzeSchema</span><span class="p">(</span><span class="n">schema_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.analyzeTable"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.analyzeTable">[docs]</a>    <span class="k">def</span> <span class="nf">analyzeTable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">estimateStatistics</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">samplePercent</span><span class="o">=</span><span class="mf">10.0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Collect stats info on a table</span>
<span class="sd">        </span>
<span class="sd">        :param schema_table_name: full table name in the format of &#39;schema.table&#39;</span>
<span class="sd">        :param estimateStatistics: will use estimate statistics if True</span>
<span class="sd">        :param samplePercent: the percentage or rows to be sampled.</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">analyzeTable</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">,</span> <span class="n">estimateStatistics</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">samplePercent</span><span class="p">))</span></div>

<div class="viewcode-block" id="PySpliceContext.export"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.export">[docs]</a>    <span class="k">def</span> <span class="nf">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">dataframe</span><span class="p">,</span>
               <span class="n">location</span><span class="p">,</span>
               <span class="n">compression</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">replicationCount</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">fileEncoding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">fieldSeparator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">quoteCharacter</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Export a dataFrame in CSV</span>

<span class="sd">        :param dataframe: (DataFrame)</span>
<span class="sd">        :param location: (str) Destination directory</span>
<span class="sd">        :param compression: (bool) Whether to compress the output or not</span>
<span class="sd">        :param replicationCount: (int) Replication used for HDFS write</span>
<span class="sd">        :param fileEncoding: (str) fileEncoding or None, defaults to UTF-8</span>
<span class="sd">        :param fieldSeparator: (str) fieldSeparator or None, defaults to &#39;,&#39;</span>
<span class="sd">        :param quoteCharacter: (str) quoteCharacter or None, defaults to &#39;&quot;&#39;</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">location</span><span class="p">,</span> <span class="n">compression</span><span class="p">,</span> <span class="n">replicationCount</span><span class="p">,</span>
                                   <span class="n">fileEncoding</span><span class="p">,</span> <span class="n">fieldSeparator</span><span class="p">,</span> <span class="n">quoteCharacter</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.exportBinary"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.exportBinary">[docs]</a>    <span class="k">def</span> <span class="nf">exportBinary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">location</span><span class="p">,</span> <span class="n">compression</span><span class="p">,</span> <span class="n">e_format</span><span class="o">=</span><span class="s1">&#39;parquet&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Export a dataFrame in binary format</span>

<span class="sd">        :param dataframe: (DataFrame)</span>
<span class="sd">        :param location: (str) Destination directory</span>
<span class="sd">        :param compression: (bool) Whether to compress the output or not</span>
<span class="sd">        :param e_format: (str) Binary format to be used, currently only &#39;parquet&#39; is supported. [Default &#39;parquet&#39;]</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">exportBinary</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">location</span><span class="p">,</span> <span class="n">compression</span><span class="p">,</span> <span class="n">e_format</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.bulkImportHFile"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.bulkImportHFile">[docs]</a>    <span class="k">def</span> <span class="nf">bulkImportHFile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">options</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Bulk Import HFile from a dataframe into a schema.table</span>

<span class="sd">        :param dataframe: (DataFrame)</span>
<span class="sd">        :param schema_table_name: (str) Full table name in the format of &quot;schema.table&quot;</span>
<span class="sd">        :param options: (Dict) Dictionary of options to be passed to --splice-properties; bulkImportDirectory is required</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">optionsMap</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">java</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">HashMap</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">options</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">optionsMap</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">bulkImportHFile</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">optionsMap</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.bulkImportHFileWithRdd"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.bulkImportHFileWithRdd">[docs]</a>    <span class="k">def</span> <span class="nf">bulkImportHFileWithRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">options</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Bulk Import HFile from an rdd into a schema.table</span>

<span class="sd">        :param rdd: (RDD) Input data</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) Full table name in the format of &quot;schema.table&quot;</span>
<span class="sd">        :param options: (Dict) Dictionary of options to be passed to --splice-properties; bulkImportDirectory is required</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bulkImportHFile</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span><span class="p">,</span>
            <span class="n">options</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.splitAndInsert"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.splitAndInsert">[docs]</a>    <span class="k">def</span> <span class="nf">splitAndInsert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">sample_fraction</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sample the dataframe, split the table, and insert a dataFrame into a schema.table.</span>
<span class="sd">        This corresponds to an insert into from select statement</span>

<span class="sd">        :param dataframe: (DataFrame) Input data</span>
<span class="sd">        :param schema_table_name: (str) Full table name in the format of &quot;schema.table&quot;</span>
<span class="sd">        :param sample_fraction: (float) A value between 0 and 1 that specifies the percentage of data in the dataFrame \</span>
<span class="sd">        that should be sampled to determine the splits. \</span>
<span class="sd">        For example, specify 0.005 if you want 0.5% of the data sampled.</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">splitAndInsert</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">sample_fraction</span><span class="p">))</span></div>

<div class="viewcode-block" id="PySpliceContext.createDataFrame"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.createDataFrame">[docs]</a>    <span class="k">def</span> <span class="nf">createDataFrame</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a dataframe from a given rdd and schema.</span>

<span class="sd">        :param rdd: (RDD) Input data</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :return: (DataFrame) The Spark DataFrame</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_session</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_generateDBSchema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">types</span><span class="o">=</span><span class="p">{}):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate the schema for create table</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># convert keys and values to uppercase in the types dictionary</span>
        <span class="n">types</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">key</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">types</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="n">db_schema</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># convert dataframe to have all uppercase column names</span>
        <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toUpper</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span>
        <span class="c1"># i contains the name and pyspark datatype of the column</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="ow">in</span> <span class="n">types</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Column </span><span class="si">{}</span><span class="s1"> is of type </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">i</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">i</span><span class="o">.</span><span class="n">dataType</span><span class="p">))</span>
                <span class="n">dt</span> <span class="o">=</span> <span class="n">types</span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">()]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dt</span> <span class="o">=</span> <span class="n">CONVERSIONS</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">dataType</span><span class="p">)]</span>
            <span class="n">db_schema</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">dt</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">db_schema</span>

    <span class="k">def</span> <span class="nf">_getCreateTableSchema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">new_schema</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parse schema for new table; if it is needed, create it</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># try to get schema and table, else set schema to splice</span>
        <span class="k">if</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">schema_table_name</span><span class="p">:</span>
            <span class="n">schema</span><span class="p">,</span> <span class="n">table</span> <span class="o">=</span> <span class="n">schema_table_name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">schema</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getConnection</span><span class="p">()</span><span class="o">.</span><span class="n">getCurrentSchemaName</span><span class="p">()</span>
            <span class="n">table</span> <span class="o">=</span> <span class="n">schema_table_name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        <span class="c1"># check for new schema</span>
        <span class="k">if</span> <span class="n">new_schema</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Creating schema </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">schema</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s1">&#39;CREATE SCHEMA </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">schema</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">schema</span><span class="p">,</span> <span class="n">table</span>

    <span class="k">def</span> <span class="nf">_dropTableIfExists</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Drop table if it exists</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tableExists</span><span class="p">(</span><span class="n">schema_and_or_table_name</span><span class="o">=</span><span class="n">schema_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Table exists. Dropping table&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropTable</span><span class="p">(</span><span class="n">schema_and_or_table_name</span><span class="o">=</span><span class="n">schema_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">)</span>

<div class="viewcode-block" id="PySpliceContext.dropTableIfExists"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.dropTableIfExists">[docs]</a>    <span class="k">def</span> <span class="nf">dropTableIfExists</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Drops a table if exists</span>
<span class="sd">        </span>
<span class="sd">        :Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                splice.dropTableIfExists(&#39;schemaName.tableName&#39;) \n</span>
<span class="sd">                # or\n</span>
<span class="sd">                splice.dropTableIfExists(&#39;schemaName&#39;, &#39;tableName&#39;)</span>

<span class="sd">        :param schema_table_name: (str) Pass the schema name in this param when passing the table_name param,</span>
<span class="sd">          or pass schemaName.tableName in this param without passing the table_name param</span>
<span class="sd">        :param table_name: (optional) (str) Table Name, used when schema_table_name contains only the schema name</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dropTableIfExists</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_jstructtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert python StructType to java StructType</span>

<span class="sd">        :param schema: PySpark StructType</span>
<span class="sd">        :return: Java Spark StructType</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="o">.</span><span class="n">parseDataType</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>

<div class="viewcode-block" id="PySpliceContext.createTable"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.createTable">[docs]</a>    <span class="k">def</span> <span class="nf">createTable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">primary_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_table_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">to_upper</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_table</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a schema.table (schema_table_name) from a dataframe</span>
<span class="sd">        </span>
<span class="sd">        :param dataframe: The Spark DataFrame to base the table off</span>
<span class="sd">        :param schema_table_name: str The schema.table to create</span>
<span class="sd">        :param primary_keys: List[str] the primary keys. Default None</span>
<span class="sd">        :param create_table_options: str The additional table-level SQL options default None</span>
<span class="sd">        :param to_upper: bool If the dataframe columns should be converted to uppercase before table creation. \</span>
<span class="sd">            If False, the table will be created with lower case columns. Default False</span>
<span class="sd">        :param drop_table: bool whether to drop the table if it exists. Default False. If False and the table exists, the function will throw an exception</span>
<span class="sd">        :return: None</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">drop_table</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dropTableIfExists</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">to_upper</span><span class="p">:</span>
            <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toUpper</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span>
        <span class="n">primary_keys</span> <span class="o">=</span> <span class="n">primary_keys</span> <span class="k">if</span> <span class="n">primary_keys</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">createTableWithSchema</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">,</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">schema</span><span class="p">,</span>
                                   <span class="n">keys</span><span class="o">=</span><span class="n">primary_keys</span><span class="p">,</span> <span class="n">create_table_options</span><span class="o">=</span><span class="n">create_table_options</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.createTableWithSchema"><a class="viewcode-back" href="../../spark.html#spark.context.PySpliceContext.createTableWithSchema">[docs]</a>    <span class="k">def</span> <span class="nf">createTableWithSchema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_table_options</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a schema.table from a schema</span>

<span class="sd">        :param schema_table_name: str The schema.table to create</span>
<span class="sd">        :param schema: (StructType) The schema that describes the columns of the table</span>
<span class="sd">        :param keys: (List[str]) The primary keys. Default None</span>
<span class="sd">        :param create_table_options: (str) The additional table-level SQL options. Default None</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">keys</span><span class="p">:</span>
            <span class="n">keys_seq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">PythonUtils</span><span class="o">.</span><span class="n">toSeq</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">keys_seq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">PythonUtils</span><span class="o">.</span><span class="n">toSeq</span><span class="p">([])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">createTable</span><span class="p">(</span>
            <span class="n">schema_table_name</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_jstructtype</span><span class="p">(</span><span class="n">schema</span><span class="p">),</span>
            <span class="n">keys_seq</span><span class="p">,</span>
            <span class="n">create_table_options</span>
        <span class="p">)</span></div></div>


<div class="viewcode-block" id="ExtPySpliceContext"><a class="viewcode-back" href="../../spark.html#spark.context.ExtPySpliceContext">[docs]</a><span class="k">class</span> <span class="nc">ExtPySpliceContext</span><span class="p">(</span><span class="n">PySpliceContext</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class implements a SplicemachineContext object from com.splicemachine.spark2 for use outside of the K8s Cloud Service</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_spliceSparkPackagesName</span> <span class="o">=</span> <span class="s2">&quot;com.splicemachine.spark2.splicemachine.*&quot;</span>

    <span class="k">def</span> <span class="nf">_splicemachineContext</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">com</span><span class="o">.</span><span class="n">splicemachine</span><span class="o">.</span><span class="n">spark2</span><span class="o">.</span><span class="n">splicemachine</span><span class="o">.</span><span class="n">SplicemachineContext</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">jdbcurl</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kafkaServers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kafkaPollTimeout</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparkSession</span><span class="p">,</span> <span class="n">JDBC_URL</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kafkaServers</span><span class="o">=</span><span class="s1">&#39;localhost:9092&#39;</span><span class="p">,</span> <span class="n">kafkaPollTimeout</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">_unit_testing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param JDBC_URL: (string) The JDBC URL Connection String for your Splice Machine Cluster</span>
<span class="sd">        :param sparkSession: (sparkContext) A SparkSession object for talking to Spark</span>
<span class="sd">        :param kafkaServers (string) Comma-separated list of Kafka broker addresses in the form host:port</span>
<span class="sd">        :param kafkaPollTimeout (int) Number of milliseconds to wait when polling Kafka</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kafkaServers</span> <span class="o">=</span> <span class="n">kafkaServers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kafkaPollTimeout</span> <span class="o">=</span> <span class="n">kafkaPollTimeout</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">sparkSession</span><span class="p">,</span> <span class="n">JDBC_URL</span><span class="p">,</span> <span class="n">_unit_testing</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Splice Machine

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>