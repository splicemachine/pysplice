{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1\n",
      "1\n",
      "after sc\n",
      "after rdd\n",
      "after collect\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import inspect\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "\n",
    "\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "print (\"t1\")\n",
    "\n",
    "print (1)\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "print(\"after sc\")\n",
    "# do something to prove it works\n",
    "rdd = sc.parallelize([1,2,3,4])\n",
    "print(\"after rdd\")\n",
    "\n",
    "# collect the RDD to a list\n",
    "list_elements = rdd.collect()\n",
    "print(\"after collect\")\n",
    "\n",
    "# print the list\n",
    "for element in list_elements:\n",
    "    print(element)\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sql started successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "%defaultDatasource jdbc:splice://agent-000001:1527/splicedb;user=splice;password=kolj$6drornUrk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PySplice creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+--------------------+---------------+-------+-----------+---------+-------+-----+------+--------+-----------+---------+------------------+\n",
      "|             TABLEID|           TABLENAME|TABLETYPE|            SCHEMAID|LOCKGRANULARITY|VERSION|COLSEQUENCE|DELIMITED|ESCAPED|LINES|STORED|LOCATION|COMPRESSION|IS_PINNED|PURGE_DELETED_ROWS|\n",
      "+--------------------+--------------------+---------+--------------------+---------------+-------+-----------+---------+-------+-----+------+--------+-----------+---------+------------------+\n",
      "|00cd0029-016c-625...|GLOBAL_LOCATION_I...|        T|08264012-016c-625...|              R|    4.0|          2|     null|   null| null|  null|    null|       none|    false|             false|\n",
      "|6da849c2-016d-470...|      TRANSFERORDERS|        T|3f46c48e-016d-470...|              R|    4.0|         19|     null|   null| null|  null|    null|       none|    false|             false|\n",
      "|38c30028-016d-470...|                   A|        T|80000000-00d2-b38...|              R|    4.0|          2|     null|   null| null|  null|    null|       none|    false|             false|\n",
      "|610e402f-016c-625...|TERRITORY_POS_ARE...|        T|08264012-016c-625...|              R|    4.0|          3|     null|   null| null|  null|    null|       none|    false|             false|\n",
      "|80000010-00d0-fd7...|    SYSCONGLOMERATES|        S|8000000d-00d0-fd7...|              R|    4.0|         -1|     null|   null| null|  null|    null|       null|    false|             false|\n",
      "|41588035-016c-625...|TERRITORY_PRODUCT...|        T|08264012-016c-625...|              R|    4.0|          3|     null|   null| null|  null|    null|       none|    false|             false|\n",
      "|21abc03b-016c-625...|TERRITORY_PAYMENT...|        T|08264012-016c-625...|              R|    4.0|          3|     null|   null| null|  null|    null|       none|    false|             false|\n",
      "|80000018-00d0-fd7...|           SYSTABLES|        S|8000000d-00d0-fd7...|              R|    4.0|         -1|     null|   null| null|  null|    null|       null|    false|             false|\n",
      "|aafec9ce-016d-470...|TO_DELIVERY_CHG_E...|        T|3f46c48e-016d-470...|              R|    4.0|         11|     null|   null| null|  null|    null|       none|    false|             false|\n",
      "|82080041-016c-625...|      DAYPART_LOOKUP|        T|08264012-016c-625...|              R|    4.0|          2|     null|   null| null|  null|    null|       none|    false|             false|\n",
      "+--------------------+--------------------+---------+--------------------+---------------+-------+-----------+---------+-------+-----+------+--------+-----------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from splicemachine.spark.context import PySpliceContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ['JDBC_HOST'] = 'agent-000001'\n",
    "jdbc_host = os.environ['JDBC_HOST']\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "#spark=SparkSession.builder.config(conf=SparkConf())\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "splicejdbc=f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk'\n",
    "\n",
    "splice = PySpliceContext(spark, splicejdbc)\n",
    "\n",
    "# Query Features\n",
    "query_results = splice.df(\"select * from sys.systables\")\n",
    "query_results.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All available PySplice functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzeSchema\n",
      "analyzeTable\n",
      "createTable\n",
      "delete\n",
      "df\n",
      "dropTable\n",
      "execute\n",
      "executeUpdate\n",
      "export\n",
      "exportBinary\n",
      "getConnection\n",
      "getSchema\n",
      "insert\n",
      "internalDf\n",
      "spark_sql_context\n",
      "tableExists\n",
      "truncateTable\n",
      "update\n",
      "upsert\n"
     ]
    }
   ],
   "source": [
    "# functions = []\n",
    "# auth_functions = []\n",
    "for i in dir(splice):\n",
    "    if i[0] != '_' and i.upper() != i and i not in ['jvm','toUpper', 'replaceDataframeSchema', 'context','jdbcurl']:\n",
    "        print(i)\n",
    "#         functions.append(i)\n",
    "# count = 0                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.sql.SQLSyntaxErrorException",
     "evalue": " Syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mjava.sql.SQLSyntaxErrorException: Syntax error: Encountered \"<EOF>\" at line 1, column 27.\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "-- drop table if exists foo;\n",
    "-- create table foo(a int, b varchar(50), c int, primary key(a));\n",
    "-- insert into foo values(1, 'test', 2);\n",
    "-- insert into foo values(2, 'foo', 3);\n",
    "-- insert into foo values(3, 'bar', 12);\n",
    "-- insert into foo values(4, 'name', 42);\n",
    "-- insert into foo values(5, 'field', 34);\n",
    "-- insert into foo values(6, 'nan', 77);\n",
    "\n",
    "-- select * from splice.foo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test analyzeSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def analyzeSchema(self, schema_name):\n",
      "        \"\"\"\n",
      "        analyze the schema\n",
      "        :param schema_name: schema name which stats info will be collected\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        return self.context.analyzeSchema(schema_name)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "NOTE: Failed without user/password in JDBC URL:\n",
      " java.sql.SQLNonTransientConnectionException: Connection authentication failure occurred.  Reason: userid or password invalid.\n",
      "<class 'NoneType'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.analyzeSchema))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    x = splice.analyzeSchema('splice')\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    x = splice.analyzeSchema('splice')\n",
    "print(type(x))\n",
    "print(x)\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test analyzeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def analyzeTable(self, schema_table_name, estimateStatistics=False, samplePercent=0.10):\n",
      "        \"\"\"\n",
      "        collect stats info on a table\n",
      "        :param schema_table_name: full table name in the format of \"schema.table\"\n",
      "        :param estimateStatistics:will use estimate statistics if True\n",
      "        :param samplePercent:  the percentage or rows to be sampled.\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        return self.context.analyzeTable(schema_table_name, estimateStatistics, samplePercent)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "NOTE: Failed without user/password in JDBC URL:\n",
      " java.sql.SQLNonTransientConnectionException: Connection authentication failure occurred.  Reason: userid or password invalid.\n",
      "<class 'NoneType'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.analyzeTable))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "# Setup\n",
    "splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "splice.execute('drop table if exists foo')\n",
    "splice.execute('create table foo(a int, b varchar(50), c int, primary key(a))')\n",
    "splice.execute('insert into foo values(1, \\'test\\', 2)')\n",
    "\n",
    "\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    x = splice.analyzeTable('splice.foo')\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    x = splice.analyzeTable('splice.foo')\n",
    "print(type(x))\n",
    "print(x)\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test createTable: CAN'T test yet: Bug was found and fixed in DBAAS-2869"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Function Definition:')\n",
    "# print(inspect.getsource(splice.createTable))\n",
    "# print('----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "# if splice: del splice\n",
    "# try:\n",
    "#     splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "#     x = splice.df('select * from splice.foo')\n",
    "#     splice.createTable(x, 'splice.foo1')\n",
    "# except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "#     print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "#     splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "#     x = splice.df('select * from splice.foo')\n",
    "#     splice.createTable(x, 'splice.foo1')\n",
    "# print(type(x))\n",
    "# print(x)\n",
    "# count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test delete: Note: table must have primary key to call a delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def delete(self, dataframe, schema_table_name):\n",
      "        \"\"\"\n",
      "        Delete records in a dataframe based on joining by primary keys from the data frame.\n",
      "        Be careful with column naming and case sensitivity.\n",
      "\n",
      "        :param dataframe: (DF) The dataframe you would like to delete\n",
      "        :param schema_table_name: (string) Splice Machine Table\n",
      "        \"\"\"\n",
      "        return self.context.delete(dataframe._jdf, schema_table_name)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "+---+-----+---+\n",
      "|  A|    B|  C|\n",
      "+---+-----+---+\n",
      "|  1| test|  2|\n",
      "|  2|  foo|  3|\n",
      "|  3|  bar| 12|\n",
      "|  4| name| 42|\n",
      "|  5|field| 34|\n",
      "|  6|  nan| 77|\n",
      "+---+-----+---+\n",
      "\n",
      "+---+-----+---+\n",
      "|  A|    B|  C|\n",
      "+---+-----+---+\n",
      "|  1| test|  2|\n",
      "|  2|  foo|  3|\n",
      "|  3|  bar| 12|\n",
      "|  4| name| 42|\n",
      "|  5|field| 34|\n",
      "|  6|  nan| 77|\n",
      "+---+-----+---+\n",
      "\n",
      "NOTE: Failed without user/password in JDBC URL:\n",
      " java.sql.SQLNonTransientConnectionException: Connection authentication failure occurred.  Reason: userid or password invalid.\n",
      "+---+-----+---+\n",
      "|  A|    B|  C|\n",
      "+---+-----+---+\n",
      "|  2|  foo|  3|\n",
      "|  3|  bar| 12|\n",
      "|  4| name| 42|\n",
      "|  5|field| 34|\n",
      "|  6|  nan| 77|\n",
      "+---+-----+---+\n",
      "\n",
      "<class 'NoneType'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.delete))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "# Setup\n",
    "splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "splice.execute('drop table if exists foo')\n",
    "splice.execute('create table foo(a int, b varchar(50), c int, primary key(a))')\n",
    "splice.execute('insert into foo values(1, \\'test\\', 2)')\n",
    "splice.execute('insert into foo values(2, \\'foo\\', 3)')\n",
    "splice.execute('insert into foo values(3, \\'bar\\', 12)')\n",
    "splice.execute('insert into foo values(4, \\'name\\', 42)')\n",
    "splice.execute('insert into foo values(5, \\'field\\', 34)')\n",
    "splice.execute('insert into foo values(6, \\'nan\\', 77)')\n",
    "splice.df('select * from foo').show()\n",
    "\n",
    "\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    splice.df('select * from splice.foo').show()\n",
    "    to_del = splice.df('select * from splice.foo where a=1')\n",
    "    x = splice.delete(to_del,'splice.foo')\n",
    "    splice.df('select * from splice.foo').show()\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    to_del = splice.df('select * from splice.foo where a=1')\n",
    "    x = splice.delete(to_del,'splice.foo')\n",
    "    splice.df('select * from splice.foo').show()\n",
    "print(type(x))\n",
    "print(x)\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def df(self, sql):\n",
      "        \"\"\"\n",
      "        Return a Spark Dataframe from the results of a Splice Machine SQL Query\n",
      "\n",
      "        :param sql: (string) SQL Query (eg. SELECT * FROM table1 WHERE column2 > 3)\n",
      "        :return: A Spark DataFrame containing the results\n",
      "        \"\"\"\n",
      "        if self._unit_testing:\n",
      "            return self.context.df(sql)\n",
      "        return DataFrame(self.context.df(sql), self.spark_sql_context)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "+---+-----+---+\n",
      "|  A|    B|  C|\n",
      "+---+-----+---+\n",
      "|  1| test|  2|\n",
      "|  2|  foo|  3|\n",
      "|  3|  bar| 12|\n",
      "|  4| name| 42|\n",
      "|  5|field| 34|\n",
      "|  6|  nan| 77|\n",
      "+---+-----+---+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "DataFrame[A: int, B: string, C: int]\n",
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  3|bar| 12|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.df))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "\n",
    "# Setup\n",
    "splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "splice.execute('drop table if exists foo')\n",
    "splice.execute('create table foo(a int, b varchar(50), c int, primary key(a))')\n",
    "splice.execute('insert into foo values(1, \\'test\\', 2)')\n",
    "splice.execute('insert into foo values(2, \\'foo\\', 3)')\n",
    "splice.execute('insert into foo values(3, \\'bar\\', 12)')\n",
    "splice.execute('insert into foo values(4, \\'name\\', 42)')\n",
    "splice.execute('insert into foo values(5, \\'field\\', 34)')\n",
    "splice.execute('insert into foo values(6, \\'nan\\', 77)')\n",
    "splice.df('select * from foo').show()\n",
    "\n",
    "\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    x = splice.df('select * from splice.foo where a=3')\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    x = splice.df('select * from splice.foo where a=3')\n",
    "print(type(x))\n",
    "print(x)\n",
    "x.show()\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dropTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def delete(self, dataframe, schema_table_name):\n",
      "        \"\"\"\n",
      "        Delete records in a dataframe based on joining by primary keys from the data frame.\n",
      "        Be careful with column naming and case sensitivity.\n",
      "\n",
      "        :param dataframe: (DF) The dataframe you would like to delete\n",
      "        :param schema_table_name: (string) Splice Machine Table\n",
      "        \"\"\"\n",
      "        return self.context.delete(dataframe._jdf, schema_table_name)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "+---+-----+---+\n",
      "|  A|    B|  C|\n",
      "+---+-----+---+\n",
      "|  1| test|  2|\n",
      "|  2|  foo|  3|\n",
      "|  3|  bar| 12|\n",
      "|  4| name| 42|\n",
      "|  5|field| 34|\n",
      "|  6|  nan| 77|\n",
      "+---+-----+---+\n",
      "\n",
      "+---+-----+---+\n",
      "|  A|    B|  C|\n",
      "+---+-----+---+\n",
      "|  1| test|  2|\n",
      "|  2|  foo|  3|\n",
      "|  3|  bar| 12|\n",
      "|  4| name| 42|\n",
      "|  5|field| 34|\n",
      "|  6|  nan| 77|\n",
      "+---+-----+---+\n",
      "\n",
      "NOTE: Failed without user/password in JDBC URL:\n",
      " java.sql.SQLNonTransientConnectionException: Connection authentication failure occurred.  Reason: userid or password invalid.\n",
      "attempting to read post drop\n",
      "drop successful\n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.delete))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Setup\n",
    "splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "splice.execute('drop table if exists foo')\n",
    "splice.execute('create table foo(a int, b varchar(50), c int, primary key(a))')\n",
    "splice.execute('insert into foo values(1, \\'test\\', 2)')\n",
    "splice.execute('insert into foo values(2, \\'foo\\', 3)')\n",
    "splice.execute('insert into foo values(3, \\'bar\\', 12)')\n",
    "splice.execute('insert into foo values(4, \\'name\\', 42)')\n",
    "splice.execute('insert into foo values(5, \\'field\\', 34)')\n",
    "splice.execute('insert into foo values(6, \\'nan\\', 77)')\n",
    "splice.df('select * from foo').show()\n",
    "\n",
    "\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    splice.df('select * from splice.foo').show()\n",
    "    splice.dropTable('splice.foo')\n",
    "    try:\n",
    "        print('attempting to read post drop')\n",
    "        splice.df('select * from splice.foo').show()\n",
    "    except:\n",
    "        print('drop successful')\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    splice.dropTable('splice.foo')\n",
    "    try:\n",
    "        print('attempting to read post drop')\n",
    "        x = splice.df('select * from splice.foo')\n",
    "    except:\n",
    "        print('drop successful')\n",
    "\n",
    "\n",
    "\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def execute(self, query_string):\n",
      "        '''\n",
      "        execute a query\n",
      "        :param query_string: (string) SQL Query (eg. SELECT * FROM table1 WHERE column2 > 3)\n",
      "        :return:\n",
      "        '''\n",
      "        return self.context.execute(query_string)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "NOTE: Failed without user/password in JDBC URL:\n",
      " java.sql.SQLNonTransientConnectionException: Connection authentication failure occurred.  Reason: userid or password invalid.\n",
      "+---+\n",
      "|  A|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.execute))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    splice.execute('drop table if exists splice.foo_two')\n",
    "    splice.execute('create table splice.foo_two(a int)')\n",
    "    splice.execute('insert into splice.foo_two values(1)')\n",
    "    splice.df('select * from splice.foo_two').show()\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    splice.execute('drop table if exists splice.foo_two')\n",
    "    splice.execute('create table splice.foo_two(a int)')\n",
    "    splice.execute('insert into splice.foo_two values(1)')\n",
    "    splice.df('select * from splice.foo_two').show()\n",
    "    \n",
    "#     x.show()\n",
    "# print(type(x))\n",
    "# print(x)\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test executeUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def executeUpdate(self, query_string):\n",
      "        '''\n",
      "        execute a dml query:(update,delete,drop,etc)\n",
      "        :param query_string: (string) SQL Query (eg. SELECT * FROM table1 WHERE column2 > 3)\n",
      "        :return:\n",
      "        '''\n",
      "        return self.context.executeUpdate(query_string)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "+---+\n",
      "|  A|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n",
      "NOTE: Failed without user/password in JDBC URL:\n",
      " java.sql.SQLNonTransientConnectionException: Connection authentication failure occurred.  Reason: userid or password invalid.\n",
      "table dropped\n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.executeUpdate))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "# Setup\n",
    "splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "splice.execute('drop table if exists foo_two')\n",
    "splice.execute('create table foo_two(a int)')\n",
    "splice.execute('insert into foo_two values(1)')\n",
    "\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    splice.df('select * from splice.foo_two').show()\n",
    "    splice.executeUpdate('drop table if exists splice.foo_two')\n",
    "    try:\n",
    "        splice.df('select * from splice.foo_two').show()\n",
    "    except:\n",
    "        print('table dropped')\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    splice.executeUpdate('drop table splice.foo_two')\n",
    "    try:\n",
    "        splice.df('select * from splice.foo_two').show()\n",
    "    except:\n",
    "        print('table dropped')\n",
    "    \n",
    "\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def export(self, dataframe, location, compression=False, replicationCount=1, fileEncoding=None,\n",
      "               fieldSeparator=None,\n",
      "               quoteCharacter=None):\n",
      "        '''\n",
      "        Export a dataFrame in CSV\n",
      "        :param dataframe:\n",
      "        :param location: Destination directory\n",
      "        :param compression: Whether to compress the output or not\n",
      "        :param replicationCount:  Replication used for HDFS write\n",
      "        :param fileEncoding: fileEncoding or null, defaults to UTF-8\n",
      "        :param fieldSeparator: fieldSeparator or null, defaults to ','\n",
      "        :param quoteCharacter: quoteCharacter or null, defaults to '\"'\n",
      "        :return:\n",
      "        '''\n",
      "        return self.context.export(dataframe._jdf, location, compression, replicationCount,\n",
      "                                   fileEncoding,\n",
      "                                   fieldSeparator, quoteCharacter)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(splice.export))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test exportBinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def exportBinary(self, dataframe, location, compression, e_format):\n",
      "        '''\n",
      "        Export a dataFrame in binary format\n",
      "        :param dataframe:\n",
      "        :param location: Destination directory\n",
      "        :param compression: Whether to compress the output or not\n",
      "        :param e_format: Binary format to be used, currently only 'parquet' is supported\n",
      "        :return:\n",
      "        '''\n",
      "        return self.context.exportBinary(dataframe._jdf, location, compression, e_format)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.exportBinary))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test getConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def getConnection(self):\n",
      "        \"\"\"\n",
      "        Return a connection to the database\n",
      "        \"\"\"\n",
      "        return self.context.getConnection()\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "<class 'py4j.java_gateway.JavaObject'>\n",
      "com.splicemachine.db.impl.jdbc.EmbedConnection40@796609640 (XID = SpliceTransaction[IDLE,null]), (SESSIONID = 1234), (DATABASE = splicedb), (DRDAID = null) \n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.getConnection))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    x = splice.getConnection()\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    x = splice.getConnection()\n",
    "print(type(x))\n",
    "print(x)\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test getSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def getSchema(self, schema_table_name):\n",
      "        \"\"\"\n",
      "        Return the schema via JDBC.\n",
      "\n",
      "        :param schema_table_name: (DF) Table name\n",
      "        \"\"\"\n",
      "        return self.context.getSchema(schema_table_name)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "+---+-----+---+\n",
      "|  A|    B|  C|\n",
      "+---+-----+---+\n",
      "|  1| test|  2|\n",
      "|  2|  foo|  3|\n",
      "|  3|  bar| 12|\n",
      "|  4| name| 42|\n",
      "|  5|field| 34|\n",
      "|  6|  nan| 77|\n",
      "+---+-----+---+\n",
      "\n",
      "NOTE: Failed without user/password in JDBC URL:\n",
      " java.sql.SQLNonTransientConnectionException: Connection authentication failure occurred.  Reason: userid or password invalid.\n",
      "<class 'py4j.java_gateway.JavaObject'>\n",
      "StructType(StructField(A,IntegerType,true), StructField(B,StringType,true), StructField(C,IntegerType,true))\n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.getSchema))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "\n",
    "# Setup\n",
    "splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "splice.execute('drop table if exists foo')\n",
    "splice.execute('create table foo(a int, b varchar(50), c int, primary key(a))')\n",
    "splice.execute('insert into foo values(1, \\'test\\', 2)')\n",
    "splice.execute('insert into foo values(2, \\'foo\\', 3)')\n",
    "splice.execute('insert into foo values(3, \\'bar\\', 12)')\n",
    "splice.execute('insert into foo values(4, \\'name\\', 42)')\n",
    "splice.execute('insert into foo values(5, \\'field\\', 34)')\n",
    "splice.execute('insert into foo values(6, \\'nan\\', 77)')\n",
    "splice.df('select * from foo').show()\n",
    "\n",
    "\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    x = splice.getSchema('splice.foo')\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    x = splice.getSchema('splice.foo')\n",
    "print(type(x))\n",
    "print(x)\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def insert(self, dataframe, schema_table_name):\n",
      "        \"\"\"\n",
      "        Insert a dataframe into a table (schema.table).\n",
      "\n",
      "        :param dataframe: (DF) The dataframe you would like to insert\n",
      "        :param schema_table_name: (string) The table in which you would like to insert the RDD\n",
      "        \"\"\"\n",
      "        # make sure column names are in the correct case\n",
      "        dataframe = self.replaceDataframeSchema(dataframe, schema_table_name)\n",
      "        return self.context.insert(dataframe._jdf, schema_table_name)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "+---+-----+---+\n",
      "|  A|    B|  C|\n",
      "+---+-----+---+\n",
      "|  1| test|  2|\n",
      "|  2|  foo|  3|\n",
      "|  3|  bar| 12|\n",
      "|  4| name| 42|\n",
      "|  5|field| 34|\n",
      "|  6|  nan| 77|\n",
      "+---+-----+---+\n",
      "\n",
      "+---+-----+---+\n",
      "|  A|    B|  C|\n",
      "+---+-----+---+\n",
      "|  1| test|  2|\n",
      "|  2|  foo|  3|\n",
      "|  3|  bar| 12|\n",
      "|  4| name| 42|\n",
      "|  5|field| 34|\n",
      "|  6|  nan| 77|\n",
      "+---+-----+---+\n",
      "\n",
      "+---+-----+---+\n",
      "|  A|    B|  C|\n",
      "+---+-----+---+\n",
      "|  1| test|  2|\n",
      "|  2|  foo|  3|\n",
      "|  3|  bar| 12|\n",
      "|  4| name| 42|\n",
      "|  5|field| 34|\n",
      "|  6|  nan| 77|\n",
      "| 11| test|  2|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.insert))\n",
    "print(f'----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "# Setup\n",
    "splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "splice.execute('drop table if exists foo')\n",
    "splice.execute('create table foo(a int, b varchar(50), c int, primary key(a))')\n",
    "splice.execute('insert into foo values(1, \\'test\\', 2)')\n",
    "splice.execute('insert into foo values(2, \\'foo\\', 3)')\n",
    "splice.execute('insert into foo values(3, \\'bar\\', 12)')\n",
    "splice.execute('insert into foo values(4, \\'name\\', 42)')\n",
    "splice.execute('insert into foo values(5, \\'field\\', 34)')\n",
    "splice.execute('insert into foo values(6, \\'nan\\', 77)')\n",
    "splice.df('select * from foo').show()\n",
    "\n",
    "\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    splice.df('select * from splice.foo').show()\n",
    "    x  = splice.df('select * from foo where a=1')\n",
    "    x = x.withColumn('A',lit(11))\n",
    "    splice.insert(x,'splice.foo')\n",
    "    splice.df('select * from splice.foo').show()\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    x  = splice.df('select * from foo where a=1')\n",
    "    x = x.withColumn('A',lit(11))\n",
    "    splice.insert(x,'splice.foo')\n",
    "    splice.df('select * from splice.foo').show()\n",
    "    \n",
    "\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test internalDf: Not sure what's wrong with this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Function Definition:')\n",
    "# print(inspect.getsource(splice.internalDf))\n",
    "# print('----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "# # Setup\n",
    "# splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "# splice.execute('drop table if exists foo')\n",
    "# splice.execute('create table foo(a int, b varchar(50), c int, primary key(a))')\n",
    "# splice.execute('insert into foo values(1, \\'test\\', 2)')\n",
    "# splice.execute('insert into foo values(2, \\'foo\\', 3)')\n",
    "# splice.execute('insert into foo values(3, \\'bar\\', 12)')\n",
    "# splice.execute('insert into foo values(4, \\'name\\', 42)')\n",
    "# splice.execute('insert into foo values(5, \\'field\\', 34)')\n",
    "# splice.execute('insert into foo values(6, \\'nan\\', 77)')\n",
    "# splice.df('select * from foo').show()\n",
    "\n",
    "\n",
    "# #Fails when you don't pass in user/password into pysplicecontext\n",
    "# if splice: del splice\n",
    "# try:\n",
    "#     splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "#     x = splice.internalDf('select * from splice.foo where a=3')\n",
    "# except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "#     print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "#     splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "#     x = splice.internalDf('select * from splice.foo where a=3')\n",
    "# print(type(x))\n",
    "# print(x)\n",
    "# x.show()\n",
    "# count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test spark_sql_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "<pyspark.sql.context.SQLContext object at 0x7f7dc30f4358>\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "+---+-----+---+\n",
      "|  A|    B|  C|\n",
      "+---+-----+---+\n",
      "|  1| test|  2|\n",
      "|  2|  foo|  3|\n",
      "|  3|  bar| 12|\n",
      "|  4| name| 42|\n",
      "|  5|field| 34|\n",
      "|  6|  nan| 77|\n",
      "+---+-----+---+\n",
      "\n",
      "should match above\n",
      "takes a failure and exception and rerun of cell to get table registered\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "print('Function Definition:')\n",
    "sqlContext = splice.spark_sql_context\n",
    "print(sqlContext)\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "\n",
    "# Setup\n",
    "splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "splice.execute('drop table if exists foo')\n",
    "splice.execute('create table foo(a int, b varchar(50), c int, primary key(a))')\n",
    "splice.execute('insert into foo values(1, \\'test\\', 2)')\n",
    "splice.execute('insert into foo values(2, \\'foo\\', 3)')\n",
    "splice.execute('insert into foo values(3, \\'bar\\', 12)')\n",
    "splice.execute('insert into foo values(4, \\'name\\', 42)')\n",
    "splice.execute('insert into foo values(5, \\'field\\', 34)')\n",
    "splice.execute('insert into foo values(6, \\'nan\\', 77)')\n",
    "splice.df('select * from foo').show()\n",
    "\n",
    "\n",
    "\n",
    "if splice: del splice\n",
    "if sqlContext: del sqlContext\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    sqlContext = splice.spark_sql_context\n",
    "    x = splice.df('select * from splice.foo')\n",
    "    x.registerTempTable('test')\n",
    "    print('should match above')\n",
    "    try:\n",
    "        sqlContext.sql('select * from test').show()\n",
    "    except:\n",
    "        print('takes a failure and exception and rerun of cell to get table registered')\n",
    "        \n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    sqlContext = splice.spark_sql_context\n",
    "    x = splice.df('select * from splice.foo')\n",
    "    x.registerTempTable('test')\n",
    "    print('should match above')\n",
    "    sqlContext.sql('select * from test').show()\n",
    "\n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tableExists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def tableExists(self, schema_table_name):\n",
      "        \"\"\"\n",
      "        Check whether or not a table exists\n",
      "\n",
      "        :param schema_table_name: (string) Table Name\n",
      "        \"\"\"\n",
      "        return self.context.tableExists(schema_table_name)\n",
      "\n",
      "----------------------------Table testing on---------------------------------\n",
      "\n",
      "+---+\n",
      "|  A|\n",
      "+---+\n",
      "|  1|\n",
      "+---+\n",
      "\n",
      "----------------------------Testing tableExists with table that exists---------------------------------\n",
      "\n",
      "NOTE: Failed without user/password in JDBC URL:\n",
      " java.sql.SQLNonTransientConnectionException: Connection authentication failure occurred.  Reason: userid or password invalid.\n",
      "Table exists: True\n",
      "----------------------------Testing tableExists with table that does NOT exists---------------------------------\n",
      "\n",
      "NOTE: Failed without user/password in JDBC URL:\n",
      " java.sql.SQLNonTransientConnectionException: Connection authentication failure occurred.  Reason: userid or password invalid.\n",
      "Table exists: False\n"
     ]
    }
   ],
   "source": [
    "## print('Function Definition:')\n",
    "print(inspect.getsource(splice.tableExists))\n",
    "print('----------------------------Table testing on---------------------------------\\n')\n",
    "\n",
    "# Setup\n",
    "splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "splice.execute('drop table if exists foo_two')\n",
    "splice.execute('create table foo_two(a int)')\n",
    "splice.execute('insert into foo_two values(1)')\n",
    "splice.df('select * from foo_two').show()\n",
    "\n",
    "if splice: del splice\n",
    "try:\n",
    "    print('----------------------------Testing tableExists with table that exists---------------------------------\\n')\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    print(splice.tableExists('splice.foo_two'))\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    a = splice.tableExists('splice.foo_two')\n",
    "    try:\n",
    "        assert(a)\n",
    "    except:\n",
    "        print(f'Should have been True, got {a}')\n",
    "    print(f'Table exists: {a}')\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# On table that does not exist\n",
    "splice.dropTable('splice.foo_two')\n",
    "if splice: del splice\n",
    "try:\n",
    "    print('----------------------------Testing tableExists with table that does NOT exists---------------------------------\\n')\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    print(splice.tableExists('splice.foo_two'))\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    a = splice.tableExists('splice.foo_two')\n",
    "    try:\n",
    "        assert(not a)\n",
    "    except:\n",
    "        print(f'Should have been False got {a}')\n",
    "    print(f'Table exists: {a}')\n",
    "    \n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test truncateTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def truncateTable(self, schema_table_name):\n",
      "        \"\"\"\n",
      "        truncate a table\n",
      "        :param schema_table_name: the full table name in the format \"schema.table_name\" which will be truncated\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        return self.context.truncateTable(schema_table_name)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "+---+\n",
      "|  A|\n",
      "+---+\n",
      "|  1|\n",
      "|  4|\n",
      "|555|\n",
      "| 33|\n",
      "|  2|\n",
      "+---+\n",
      "\n",
      "NOTE: Failed without user/password in JDBC URL:\n",
      " java.sql.SQLNonTransientConnectionException: Connection authentication failure occurred.  Reason: userid or password invalid.\n",
      "+---+\n",
      "|  A|\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.truncateTable))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "splice.execute('drop table if exists foo_two')\n",
    "splice.execute('create table foo_two(a int)')\n",
    "splice.execute('insert into foo_two values(1)')\n",
    "splice.execute('insert into foo_two values(4)')\n",
    "splice.execute('insert into foo_two values(555)')\n",
    "splice.execute('insert into foo_two values(33)')\n",
    "splice.execute('insert into foo_two values(2)')\n",
    "splice.df('select * from foo_two').show()\n",
    "\n",
    "\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    splice.truncateTable('splice.foo_two')\n",
    "    splice.df('select * from foo_two').show()\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    splice.truncateTable('splice.foo_two')\n",
    "    splice.df('select * from foo_two').show()\n",
    "\n",
    "    \n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test update: \n",
    "### Note: table must have primary key to call a update\n",
    "#### Also Note: You cannot update the primary key (as expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def update(self, dataframe, schema_table_name):\n",
      "        \"\"\"\n",
      "        Update data from a dataframe for a specified schema_table_name (schema.table).\n",
      "        The keys are required for the update and any other columns provided will be updated\n",
      "        in the rows.\n",
      "\n",
      "        :param dataframe: (DF) The dataframe you would like to update\n",
      "        :param schema_table_name: (string) Splice Machine Table\n",
      "        \"\"\"\n",
      "        # make sure column names are in the correct case\n",
      "        dataframe = self.replaceDataframeSchema(dataframe, schema_table_name)\n",
      "        return self.context.update(dataframe._jdf, schema_table_name)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "|  5|  6|\n",
      "+---+---+\n",
      "\n",
      "\n",
      "What the table should look like:\n",
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  4|\n",
      "|  3|  8|\n",
      "|  5| 12|\n",
      "+---+---+\n",
      "\n",
      "NOTE: Failed without user/password in JDBC URL:\n",
      " java.sql.SQLNonTransientConnectionException: Connection authentication failure occurred.  Reason: userid or password invalid.\n",
      "\n",
      "What the table should look like:\n",
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  4|\n",
      "|  3|  8|\n",
      "|  5| 12|\n",
      "+---+---+\n",
      "\n",
      "The table below should match the table above:\n",
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  4|\n",
      "|  3|  8|\n",
      "|  5| 12|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.update))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "splice.execute('drop table if exists foo_two')\n",
    "splice.execute('create table foo_two(a int, b int, primary key(a))')\n",
    "splice.execute('insert into foo_two values(1, 2)')\n",
    "splice.execute('insert into foo_two values(3, 4)')\n",
    "splice.execute('insert into foo_two values(5, 6)')\n",
    "splice.df('select * from foo_two').show()\n",
    "\n",
    "\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    x = splice.df('select * from splice.foo_two')\n",
    "    print('\\nWhat the table should look like:')\n",
    "    x = x.withColumn('B', x['B']*2)\n",
    "    x.show()\n",
    "    splice.update(x, 'splice.foo_two')\n",
    "    print('The table below should match the table above:')\n",
    "    splice.df('select * from foo_two').show()\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    x = splice.df('select * from splice.foo_two')\n",
    "    print('\\nWhat the table should look like:')\n",
    "    x = x.withColumn('B', x['B']*2)\n",
    "    x.show()\n",
    "    splice.update(x, 'splice.foo_two')\n",
    "    print('The table below should match the table above:')\n",
    "    splice.df('select * from foo_two').show()\n",
    "    \n",
    "count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Definition:\n",
      "    def upsert(self, dataframe, schema_table_name):\n",
      "        \"\"\"\n",
      "        Upsert the data from a dataframe into a table (schema.table).\n",
      "\n",
      "        :param dataframe: (DF) The dataframe you would like to upsert\n",
      "        :param schema_table_name: (string) The table in which you would like to upsert the RDD\n",
      "        \"\"\"\n",
      "        # make sure column names are in the correct case\n",
      "        dataframe = self.replaceDataframeSchema(dataframe, schema_table_name)\n",
      "        return self.context.upsert(dataframe._jdf, schema_table_name)\n",
      "\n",
      "----------------------------Testing Function---------------------------------\n",
      "\n",
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "|  5|  6|\n",
      "+---+---+\n",
      "\n",
      "\n",
      "The table should have the following rows appended:\n",
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  2|  2|\n",
      "|  6|  4|\n",
      "| 10|  6|\n",
      "+---+---+\n",
      "\n",
      "The table below should have the rows from above appended:\n",
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  2|  2|\n",
      "|  3|  4|\n",
      "|  5|  6|\n",
      "|  6|  4|\n",
      "| 10|  6|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Function Definition:')\n",
    "print(inspect.getsource(splice.upsert))\n",
    "print('----------------------------Testing Function---------------------------------\\n')\n",
    "\n",
    "splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "splice.execute('drop table if exists foo_two')\n",
    "splice.execute('create table foo_two(a int, b int, primary key(a))')\n",
    "splice.execute('insert into foo_two values(1, 2)')\n",
    "splice.execute('insert into foo_two values(3, 4)')\n",
    "splice.execute('insert into foo_two values(5, 6)')\n",
    "splice.df('select * from foo_two').show()\n",
    "\n",
    "\n",
    "#Fails when you don't pass in user/password into pysplicecontext\n",
    "if splice: del splice\n",
    "try:\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb')\n",
    "    x = splice.df('select * from splice.foo_two')\n",
    "    print('\\nThe table should have the following rows appended:')\n",
    "    x = x.withColumn('A', x['A']*2)\n",
    "    x.show()\n",
    "    splice.upsert(x, 'splice.foo_two')\n",
    "    print('The table below should have the rows from above appended:')\n",
    "    splice.df('select * from foo_two').show()\n",
    "except Exception as e:\n",
    "#     auth_functions.append(functions[count])\n",
    "    print('NOTE: Failed without user/password in JDBC URL:\\n', e.java_exception)\n",
    "    splice = PySpliceContext(spark, f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=kolj$6drornUrk')\n",
    "    x = splice.df('select * from splice.foo_two')\n",
    "    print('\\nThe table should have the following rows appended:')\n",
    "    x = x.withColumn('A', x['A']*2)\n",
    "    x.show()\n",
    "    splice.upsert(x, 'splice.foo_two')\n",
    "    print('The table below should have the rows from above appended:')\n",
    "    splice.df('select * from foo_two').show()\n",
    "    \n",
    "count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
